{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d0a5695-a57a-44aa-8d99-201e7e6d1415",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"transformers.utils\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"thinc.shims\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"spacy_transformers.layers\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"spacy_transformers.layers\")\n",
    "#warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"spacy.util\")\n",
    "\n",
    "import torch\n",
    "import spacy\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datasets\n",
    "from spacy import displacy\n",
    "from functools import reduce\n",
    "import utils\n",
    "import rebel_spacy\n",
    "\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "from config import load_config\n",
    "from graphs import (\n",
    "    explode_columns,\n",
    "    run_query,\n",
    "    get_all_relationships,\n",
    "    get_all_nodes,\n",
    "    cleanup_database,\n",
    "    create_relationship,\n",
    "    create_node,\n",
    ")\n",
    "\n",
    "#from spacy_custom import get_relations\n",
    "from spacy.tokens import Doc\n",
    "from wasabi import msg\n",
    "\n",
    "from coref import resolve_references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48ad8e21-ac2f-4236-9781-fa3b30bbe1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch_device = \"gpu\" \n",
    "    rebel_device = 0\n",
    "else:\n",
    "    torch_device =\"cpu\"\n",
    "    rebel_device = -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7499677f-6f38-4292-8ff8-25be9d96ebf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "config, secrets = load_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3366a82-4abd-426e-94ec-5d30a1ab2a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "# Load a dataset and retrieve just a sample in `ds`\n",
    "roc18_ds = datasets.load_dataset(\"igormorgado/ROCStories2018\")\n",
    "ds = roc18_ds['train'].select(range(10))\n",
    "sample = ds[0]\n",
    "text = utils.rocstory(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9b020f3-8190-406f-ade9-1fc01ae299a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\n",
      "============================= Pipeline Overview =============================\u001b[0m\n",
      "\n",
      "#   Component         Assigns               Requires   Scores             Retokenizes\n",
      "-   ---------------   -------------------   --------   ----------------   -----------\n",
      "0   tok2vec           doc.tensor                                          False      \n",
      "                                                                                     \n",
      "1   tagger            token.tag                        tag_acc            False      \n",
      "                                                                                     \n",
      "2   parser            token.dep                        dep_uas            False      \n",
      "                      token.head                       dep_las                       \n",
      "                      token.is_sent_start              dep_las_per_type              \n",
      "                      doc.sents                        sents_p                       \n",
      "                                                       sents_r                       \n",
      "                                                       sents_f                       \n",
      "                                                                                     \n",
      "3   attribute_ruler                                                       False      \n",
      "                                                                                     \n",
      "4   lemmatizer        token.lemma                      lemma_acc          False      \n",
      "                                                                                     \n",
      "5   ner               doc.ents                         ents_f             False      \n",
      "                      token.ent_iob                    ents_p                        \n",
      "                      token.ent_type                   ents_r                        \n",
      "                                                       ents_per_type                 \n",
      "\n",
      "\u001b[38;5;2mâœ” No problems found.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "doc = nlp(text)\n",
    "nlp.analyze_pipes(pretty=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f1e2036-c6e2-4a31-9e5e-1f1cf6a2d16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp_coref = spacy.load(\"en_coreference_web_trf\")\n",
    "# use replace_listeners for the coref components\n",
    "#nlp_coref.replace_listeners(\"transformer\", \"coref\", [\"model.tok2vec\"])\n",
    "#nlp_coref.replace_listeners(\"transformer\", \"span_resolver\", [\"model.tok2vec\"])\n",
    "#nlp.add_pipe(\"merge_entities\")\n",
    "#nlp.add_pipe(\"coref\", source=nlp_coref)\n",
    "#nlp.add_pipe(\"span_resolver\", source=nlp_coref)\n",
    "#nlp_coref.analyze_pipes(pretty=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cea39b98-014c-4161-95e3-7cbf68244e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc_nlp = nlp(text)\n",
    "#doc_coref = nlp_coref(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cac82f5b-e875-445e-8802-10f79cd19e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_to_json(token, sentence_id):\n",
    "    token_dict = {\n",
    "        \"id\": f\"{sentence_id}_{token.i}\",\n",
    "        \"sentence_id\": sentence_id,\n",
    "        \"text\": token.text,\n",
    "        \"lemma\": token.lemma_,\n",
    "        \"pos\": token.pos_,\n",
    "        \"tag\": token.tag_,\n",
    "        \"dep\": token.dep_,\n",
    "        \"ent_type\": token.ent_type_,\n",
    "        \"is_alpha\": token.is_alpha,\n",
    "        \"is_stop\": token.is_stop,\n",
    "        \"is_sent_start\": token.is_sent_start,\n",
    "        \"head\": f\"{sentence_id}_{token.head.i}\",\n",
    "    }\n",
    "    return token_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83eabd8a-40f9-4b5e-a81d-56e7dd7067f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not using rebel for now to extract Relations.\n",
    "# spacy_lm = 'en_core_web_sm'\n",
    "# relext_pipeline = spacy.load(spacy_lm, disable=['ner', 'lemmatizer', 'attribute_rules', 'tagger'])\n",
    "\n",
    "# rebel_config_params = {\n",
    "#     'device': rebel_device,\n",
    "#     'model_name': 'Babelscape/rebel-large'\n",
    "# }\n",
    "# rebel_comp = relext_pipeline.add_pipe(\"rebel\", config=rebel_config_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "684f3139-98f1-4abb-bf74-2ae6f8c48a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc[doc.ents[0].start:doc.ents[0].end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "262f4fe6-8d7f-49b9-a626-95f88e953286",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_token_query() -> str:\n",
    "    query = \"\"\"\n",
    "        CREATE (t:Token{labels} {{\n",
    "            id: $id,\n",
    "            sentence_id: $sentence_id,\n",
    "            text: $text,\n",
    "            pos: $pos,\n",
    "            tag: $tag,\n",
    "            lemma: $lemma,\n",
    "            is_alpha: $is_alpha,\n",
    "            is_stop: $is_stop,\n",
    "            is_sent_start: $is_sent_start\n",
    "        }})\n",
    "        RETURN t\n",
    "    \"\"\"\n",
    "    return query\n",
    "\n",
    "def build_dependency_query() -> str:\n",
    "    query = \"\"\"\n",
    "        MATCH (t1:Token {id: $id}), (t2:Token {id: $head})\n",
    "        CREATE (t1)-[:DEPENDS_ON {type: $dep}]->(t2)\n",
    "    \"\"\"\n",
    "    return query\n",
    "\n",
    "def build_sentence_query() -> str:\n",
    "    query = \"\"\"\n",
    "        MATCH (r1:Token {id: $id}), (r2:Token {id: $next})\n",
    "        CREATE (r1)-[:NEXT_SENTENCE]->(r2)\n",
    "    \"\"\"\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b9616ba6-a720-4680-9da2-621d3b67b4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependency graph created in Neo4j with 5 sentences linked.\n"
     ]
    }
   ],
   "source": [
    "# Connect to Neo4j\n",
    "driver = GraphDatabase.driver(config.neo4j.uri, auth=(config.neo4j.username, secrets.neo4j_password))\n",
    "with driver.session() as session:\n",
    "    # Clear existing graph\n",
    "    session.execute_write(cleanup_database)\n",
    "\n",
    "    roots = []\n",
    "    sentence_id = 0\n",
    "\n",
    "    for sentence_id, sent in enumerate(doc.sents):\n",
    "        root = None\n",
    "     \n",
    "        # Create nodes for each token in the sentence\n",
    "        for token in sent:\n",
    "            query = build_token_query()\n",
    "            data = token_to_json(token, sentence_id)\n",
    "            ent_type = data.get(\"ent_type\")\n",
    "            if ent_type:\n",
    "                query = query.format(labels=f\":{ent_type}\")\n",
    "            else:\n",
    "                query = query.format(labels=\"\")\n",
    "\n",
    "            node = session.execute_write(run_query, query, data)\n",
    "            \n",
    "            if token.dep_ == \"ROOT\":\n",
    "                roots.append(node)\n",
    "                   \n",
    "        # Create relationships based on dependencies within the sentence\n",
    "        for token in sent:\n",
    "            if token.head.i != token.i:  # Exclude root\n",
    "                query = build_dependency_query()\n",
    "                data = token_to_json(token, sentence_id)\n",
    "                session.execute_write(run_query, query, data)\n",
    "    \n",
    "    # Create relationships between sentence roots\n",
    "    roots_df = pd.concat(roots, ignore_index=True)\n",
    "    root_ids = list(roots_df['t'].apply(lambda x: x.get('id')))\n",
    "    for id_, next_ in zip(root_ids[:-1], root_ids[1:]):\n",
    "        query = build_sentence_query()\n",
    "        data = {\"id\": id_, \"next\": next_}\n",
    "        session.execute_write(run_query, query, data)\n",
    "\n",
    "    driver.close()\n",
    "    print(f\"Dependency graph created in Neo4j with {len(root_ids)} sentences linked.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f808bf5b-ee93-4e13-b52d-695c0a231ebf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
