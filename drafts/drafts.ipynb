{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20ffe674-6c5f-4942-aadd-6ce223f24006",
   "metadata": {},
   "source": [
    "# Drafts\n",
    "\n",
    "These are just drafts. Often code that was removed from other notebooks and put here to be kept for some unknown reason. Code here probably do not work as standalone since it will need for functions that were changed, removed or merged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9334baf0-5061-4617-b4fd-bda6a2cf83b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__cinit__() takes exactly 3 positional arguments (4 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m     23\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis  is  a   sample   text   with   extra   spaces.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 24\u001b[0m without_spaces, with_spaces \u001b[38;5;241m=\u001b[39m \u001b[43mtokenize_with_and_without_spaces\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokens without spaces:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m([token\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m without_spaces])\n",
      "Cell \u001b[0;32mIn[3], line 16\u001b[0m, in \u001b[0;36mtokenize_with_and_without_spaces\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     12\u001b[0m tokens_with_spaces \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m doc:\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# Create a new Token object without trailing space\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m     no_space_token \u001b[38;5;241m=\u001b[39m \u001b[43mToken\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     tokens_without_spaces\u001b[38;5;241m.\u001b[39mappend(no_space_token)\n\u001b[1;32m     18\u001b[0m     tokens_with_spaces\u001b[38;5;241m.\u001b[39mappend(token)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.10/lib/python3.11/site-packages/spacy/tokens/token.pyx:104\u001b[0m, in \u001b[0;36mspacy.tokens.token.Token.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __cinit__() takes exactly 3 positional arguments (4 given)"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from typing import List, Tuple\n",
    "from spacy.tokens import Token\n",
    "\n",
    "def tokenize_with_and_without_spaces(text: str) -> Tuple[List[Token], List[Token]]:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    \n",
    "    # Disable all pipeline components except tokenizer\n",
    "    doc = nlp(text, disable=[\"parser\", \"tagger\", \"ner\"])\n",
    "    \n",
    "    tokens_without_spaces = []\n",
    "    tokens_with_spaces = []\n",
    "    \n",
    "    for token in doc:\n",
    "        # Create a new Token object without trailing space\n",
    "        no_space_token = Token(doc.vocab, token.text.strip(), token.i, token.i + 1)\n",
    "        tokens_without_spaces.append(no_space_token)\n",
    "        tokens_with_spaces.append(token)\n",
    "    \n",
    "    return tokens_without_spaces, tokens_with_spaces\n",
    "\n",
    "# Example usage\n",
    "text = \"This  is  a   sample   text   with   extra   spaces.\"\n",
    "without_spaces, with_spaces = tokenize_with_and_without_spaces(text)\n",
    "\n",
    "print(\"Tokens without spaces:\")\n",
    "print([token.text for token in without_spaces])\n",
    "print(\"\\nTokens with spaces:\")\n",
    "print([token.text_with_ws for token in with_spaces])\n",
    "\n",
    "print(\"\\nOne-to-one mapping:\")\n",
    "for token1, token2 in zip(without_spaces, with_spaces):\n",
    "    print(f\"'{token1.text}' <-> '{token2.text_with_ws}'\")\n",
    "\n",
    "# Demonstrating access to Token attributes\n",
    "print(\"\\nDemonstrating Token attributes:\")\n",
    "for token in with_spaces[:3]:  # Just showing first 3 tokens\n",
    "    print(f\"Token: '{token.text_with_ws}'\")\n",
    "    print(f\"  Is alpha: {token.is_alpha}\")\n",
    "    print(f\"  Is punctuation: {token.is_punct}\")\n",
    "    print(f\"  Whitespace: '{token.whitespace_}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "786e746a-8072-4fc3-8e06-ba93e947acbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'<class 'spacy.tokens.token.Token'>' '0' 'Christian' '0' '383' 'ORG'\n",
      "'<class 'spacy.tokens.token.Token'>' '1' 'Drosten' '0' '383' 'ORG'\n",
      "'<class 'spacy.tokens.token.Token'>' '2' 'works' '0' '0' ''\n",
      "'<class 'spacy.tokens.token.Token'>' '3' 'in' '0' '0' ''\n",
      "'<class 'spacy.tokens.token.Token'>' '4' 'Germany' '0' '384' 'GPE'\n",
      "'<class 'spacy.tokens.token.Token'>' '5' '.' '0' '0' ''\n",
      "'<class 'spacy.tokens.token.Token'>' '6' 'He' '0' '0' ''\n",
      "'<class 'spacy.tokens.token.Token'>' '7' 'likes' '0' '0' ''\n",
      "'<class 'spacy.tokens.token.Token'>' '8' 'to' '0' '0' ''\n",
      "'<class 'spacy.tokens.token.Token'>' '9' 'work' '0' '0' ''\n",
      "'<class 'spacy.tokens.token.Token'>' '10' 'for' '0' '0' ''\n",
      "'<class 'spacy.tokens.token.Token'>' '11' 'Google' '0' '383' 'ORG'\n",
      "'<class 'spacy.tokens.token.Token'>' '12' '.' '0' '0' ''\n"
     ]
    }
   ],
   "source": [
    "# Show all relations\n",
    "for token in doc:\n",
    "    print(f\"'{type(token)}' '{token.i}' '{token.text}' '{token.ent_id}' '{token.ent_type}' '{token.ent_type_}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "687ee620-84a3-41ec-9123-f5617a3f8c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens without spaces:\n",
      "['This', 'is', 'a', 'sample', 'text', 'with', 'extra', 'spaces', '.']\n",
      "\n",
      "Tokens with spaces:\n",
      "['This ', 'is ', 'a ', 'sample ', 'text ', 'with ', 'extra ', 'spaces', '.']\n",
      "\n",
      "One-to-one mapping:\n",
      "'This' <-> 'This '\n",
      "'is' <-> 'is '\n",
      "'a' <-> 'a '\n",
      "'sample' <-> 'sample '\n",
      "'text' <-> 'text '\n",
      "'with' <-> 'with '\n",
      "'extra' <-> 'extra '\n",
      "'spaces' <-> 'spaces'\n",
      "'.' <-> '.'\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from typing import List, Tuple\n",
    "\n",
    "def tokenize_with_and_without_spaces(text: str) -> Tuple[List[str], List[str]]:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    \n",
    "    # Disable all pipeline components except tokenizer\n",
    "    doc = nlp(text, disable=[\"parser\", \"tagger\", \"ner\"])\n",
    "    \n",
    "    tokens_without_spaces = []\n",
    "    tokens_with_spaces = []\n",
    "    \n",
    "    for token in doc:\n",
    "        tokens_without_spaces.append(token.text.strip())\n",
    "        tokens_with_spaces.append(token.text_with_ws)\n",
    "    \n",
    "    return tokens_without_spaces, tokens_with_spaces\n",
    "\n",
    "# Example usage\n",
    "text = \"This is a sample text with extra spaces.\"\n",
    "without_spaces, with_spaces = tokenize_with_and_without_spaces(text)\n",
    "\n",
    "print(\"Tokens without spaces:\")\n",
    "print(without_spaces)\n",
    "print(\"\\nTokens with spaces:\")\n",
    "print(with_spaces)\n",
    "\n",
    "print(\"\\nOne-to-one mapping:\")\n",
    "for token1, token2 in zip(without_spaces, with_spaces):\n",
    "    print(f\"'{token1}' <-> '{token2}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d54c37b-0505-4063-a3b8-3c33201a391c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cypher_query(nodes, relationships):\n",
    "    query = []\n",
    "    \n",
    "    # Create node creation statements\n",
    "    for pos, words in nodes.items():\n",
    "        for word in words:\n",
    "            query.append(f\"CREATE (:{pos} {{name: '{word}'}})\")\n",
    "    \n",
    "    # Create relationship statements\n",
    "    for governor, relation, dependent in relationships:\n",
    "        query.append(f\"MATCH (a {{name: '{governor}'}}), (b {{name: '{dependent}'}}) CREATE (a)-[:{relation}]->(b)\")\n",
    "    \n",
    "    return \";\\n\".join(query) + \";\"\n",
    "\n",
    "\n",
    "# Example usage\n",
    "text = \"The quick brown fox jumps over the lazy dog. It chases mice in the garden.\"\n",
    "nodes, relationships = analyze_semantic_relationships(text)\n",
    "cypher_query = generate_cypher_query(nodes, relationships)\n",
    "\n",
    "print(\"Cypher Query:\")\n",
    "print(cypher_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "611f79c1-a135-4146-ad36-81e64ddaca62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import cudf\n",
    "import cugraph\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Create cuDF DataFrames\n",
    "nodes_df = cudf.DataFrame(data['nodes'])\n",
    "edges_df = cudf.DataFrame(data['relationships'])\n",
    "\n",
    "# Create cuGraph graph\n",
    "G_cu = cugraph.Graph()\n",
    "G_cu.from_cudf_edgelist(edges_df, source='source', destination='target')\n",
    "\n",
    "# Convert to NetworkX graph for visualization\n",
    "G_nx = nx.from_pandas_edgelist(edges_df.to_pandas(), 'source', 'target', edge_attr=True)\n",
    "\n",
    "# Add node attributes\n",
    "for node in data['nodes']:\n",
    "    G_nx.nodes[node['id']]['type'] = node['type']\n",
    "    G_nx.nodes[node['id']]['name'] = node['name']\n",
    "    G_nx.nodes[node['id']]['age'] = node['age']\n",
    "\n",
    "# Visualize the graph\n",
    "pos = nx.spring_layout(G_nx)\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Draw nodes\n",
    "node_colors = {'PERSON': 'skyblue', 'EMPLOYEE': 'lightgreen', 'COMPANY': 'salmon'}\n",
    "for node_type in node_colors:\n",
    "    nx.draw_networkx_nodes(G_nx, pos, \n",
    "                           nodelist=[n for n, d in G_nx.nodes(data=True) if d['type'] == node_type],\n",
    "                           node_color=node_colors[node_type], \n",
    "                           node_size=3000, \n",
    "                           alpha=0.8)\n",
    "\n",
    "# Draw edges\n",
    "nx.draw_networkx_edges(G_nx, pos, width=2, alpha=0.5, edge_color='gray')\n",
    "\n",
    "# Add labels\n",
    "nx.draw_networkx_labels(G_nx, pos, {n: d['name'] for n, d in G_nx.nodes(data=True)})\n",
    "edge_labels = nx.get_edge_attributes(G_nx, 'type')\n",
    "nx.draw_networkx_edge_labels(G_nx, pos, edge_labels=edge_labels)\n",
    "\n",
    "plt.title(\"Graph Visualization\", fontsize=16)\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print some graph information\n",
    "print(f\"Number of nodes: {G_cu.number_of_vertices()}\")\n",
    "print(f\"Number of edges: {G_cu.number_of_edges()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e808259-a4d4-4b14-8e31-9b876b797096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_graph_from_neo4j():\n",
    "    with GraphDatabase.driver(URI, auth=(USERNAME, PASSWORD)) as driver:\n",
    "        with driver.session() as session:\n",
    "            # This query assumes your graph has 'Node' labels and 'RELATIONSHIP' type\n",
    "            # Modify as needed to match your Neo4j graph structure\n",
    "            result = session.run(\"\"\"\n",
    "                MATCH (source:Node)-[r:RELATIONSHIP]->(target:Node)\n",
    "                RETURN id(source) AS source, id(target) AS target, type(r) AS type\n",
    "            \"\"\")\n",
    "            return [(record[\"source\"], record[\"target\"], record[\"type\"]) for record in result]\n",
    "\n",
    "def main():\n",
    "    # Read the graph from Neo4j\n",
    "    edges = read_graph_from_neo4j()\n",
    "    \n",
    "    # Create a cuDF DataFrame from the edges\n",
    "    df = cudf.DataFrame(edges, columns=['source', 'target', 'type'])\n",
    "    \n",
    "    # Create a cuGraph object\n",
    "    G_cugraph = cugraph.Graph()\n",
    "    G_cugraph.from_cudf_edgelist(df, source='source', destination='target', edge_attr='type')\n",
    "    \n",
    "    # Convert cuGraph to NetworkX for visualization\n",
    "    G_nx = cugraph.utilities.convert_to_nx(G_cugraph)\n",
    "    \n",
    "    # Plot the graph\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    pos = nx.spring_layout(G_nx)\n",
    "    nx.draw(G_nx, pos, with_labels=True, node_color='lightblue', \n",
    "            node_size=500, font_size=10, font_weight='bold')\n",
    "    edge_labels = nx.get_edge_attributes(G_nx, 'type')\n",
    "    nx.draw_networkx_edge_labels(G_nx, pos, edge_labels=edge_labels)\n",
    "    \n",
    "    plt.title(\"Neo4j Graph Visualization\")\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d2b532-4e28-4fc2-8677-f693b5e0d11a",
   "metadata": {},
   "source": [
    "### HANDMADE Spacy_component.py\n",
    "\n",
    "This is a rewrite of `spacy_component.py` from REBEL for a single sentence. The\n",
    "main objective is to verify what is happening internaly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e56385a-36c6-4c28-ab66-986d86170372",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd79476e-330f-4ece-9d9a-8b3540f9bbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wiki_id(item):\n",
    "  try:\n",
    "    url = f\"https://www.wikidata.org/w/api.php?action=wbsearchentities&search={item}&language=en&format=json\"\n",
    "    data = requests.get(url).json()\n",
    "    # Return the first id (Could upgrade this in the future)\n",
    "    return data['search'][0]['id']\n",
    "  except (KeyError, IndexError):\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b6a96a4-366e-4422-90c7-1a00a840889f",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Christian Drosten works in Germany. He likes to work for Google.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "544ad438-db08-402d-b5db-b08f95b8752e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = doc.sents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2e8bc1-ecf9-425e-af67-b4d0d2acb202",
   "metadata": {},
   "source": [
    "### `_generate_triplets(sents: List[spacy.Span]) -> List[List[dict]]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ebc38c6-8c27-4e6d-94ab-90680c0c2060",
   "metadata": {},
   "outputs": [],
   "source": [
    "triplet_extractor = pipeline(\n",
    "    task=\"text2text-generation\",\n",
    "    model=rebel_config_params['model_name'],\n",
    "    tokenizer=rebel_config_params['model_name'],\n",
    "    device=rebel_config_params['device'],\n",
    "    num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f7bfee5-ab7c-4504-b82b-aa6ddd514bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_ids = triplet_extractor(\n",
    "    [sent.text for sent in sents],\n",
    "    return_tensors=True,\n",
    "    return_text=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e8ae0c87-abc9-4219-a31f-18082584ca84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'generated_token_ids': tensor([    0, 50267,  2412, 26513, 11681,  1437, 50266,  1600,  1437, 50265,\n",
       "           247,     9,  8860,     2])}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "95be6e4f-2b88-4ef1-99fc-727e6c66bafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_texts = triplet_extractor.tokenizer.batch_decode(\n",
    "    [\n",
    "        out[\"generated_token_ids\"]\n",
    "        for out in output_ids\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "336ec874-9a0e-4a1e-b32b-a1cf779fdcf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s><triplet> Christian Drosten <subj> Germany <obj> country of citizenship</s>'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "db4bce04-7a64-487d-b6dc-928e87f18363",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_triplets = [\n",
    "    spacy_component.extract_triplets(text)\n",
    "    for text in extracted_texts\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "26b44617-1b9f-42f4-801b-a7686239aef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'head': 'Christian Drosten',\n",
       "  'type': 'country of citizenship',\n",
       "  'tail': 'Germany'}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_triplets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f4b59734-be1d-4c8b-848d-a1426c758249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'head': 'Christian Drosten',\n",
       "   'type': 'country of citizenship',\n",
       "   'tail': 'Germany'}],\n",
       " [{'head': 'Google', 'type': 'subsidiary', 'tail': 'Google+'}]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_triplets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1319d5-ac1d-44ec-b0bb-5325063f689a",
   "metadata": {},
   "source": [
    "### Set annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6b6c4097-cb51-4845-b43d-e9ca9385ca3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f39d845d-bedf-479e-9864-de6c2665fc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = doc.text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ea95926f-c4f9-493b-9a58-d39653efc974",
   "metadata": {},
   "outputs": [],
   "source": [
    "extension_name = \"myrel\"\n",
    "if not doc.has_extension(extension_name):\n",
    "    doc.set_extension(extension_name, default={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c5d95cb3-e692-4c1b-b4c9-baccabddc28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "triplets = extracted_triplets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "69fd24ba-6b4d-4a01-995a-a8bc4aaa6b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_annotations(doc, triplets):\n",
    "    for triplet in triplets:\n",
    "        if triplet['head'] == triplet['tail']:\n",
    "            print(\"Empty\")\n",
    "            continue\n",
    "    \n",
    "        head_span = re.search(triplet[\"head\"], doc.text)\n",
    "        tail_span = re.search(triplet[\"tail\"], doc.text)\n",
    "    \n",
    "        if not head_span or not tail_span:\n",
    "            continue\n",
    "    \n",
    "        index_value = \"\".join(triplet.values())\n",
    "        index = hashlib.sha1(index_value.encode('utf-8')).hexdigest()\n",
    "        myrel = doc._.get(extension_name)\n",
    "\n",
    "        if index not in myrel:\n",
    "            head_id = get_wiki_id(triplet[\"head\"])\n",
    "            tail_id = get_wiki_id(triplet[\"tail\"])\n",
    "            myrel[index] = {\n",
    "                \"relation\": triplet[\"type\"],\n",
    "                \"head_span\": {\n",
    "                    \"text\": triplet[\"head\"],\n",
    "                    \"id\": head_id\n",
    "                },\n",
    "                \"tail_span\": {\n",
    "                    \"text\": triplet[\"tail\"],\n",
    "                    \"id\": tail_id\n",
    "                }\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d59fea46-e847-4791-9f3c-3b0293827745",
   "metadata": {},
   "outputs": [],
   "source": [
    "for extracted_triplet in extracted_triplets:\n",
    "    set_annotations(doc, extracted_triplet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "25051803-5c65-4bcb-bb50-af7e8f9ddd35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2687c27c21e64f35b1c45121a65f89bd3cf28db1': {'relation': 'country of citizenship',\n",
       "  'head_span': {'text': 'Christian Drosten', 'id': 'Q1079331'},\n",
       "  'tail_span': {'text': 'Germany', 'id': 'Q183'}},\n",
       " '4e1196959822f17bb9fae4ea65a1788f73eb8e25': {'relation': 'subsidiary',\n",
       "  'head_span': {'text': 'Google', 'id': 'Q95'},\n",
       "  'tail_span': {'text': 'Google+', 'id': 'Q95'}}}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc._.myrel"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
